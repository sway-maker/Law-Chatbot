# Python imports
import os
import re
import jieba
from docx import Document
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
import faiss
from transformers import pipeline, AutoTokenizer
from tqdm import tqdm
import torch
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics.pairwise import cosine_similarity
import gradio as gr

# åµæ¸¬ GPU/CPU
device = "cuda" if torch.cuda.is_available() else "cpu"

# Gradio ä»‹é¢ CSS èˆ‡ HTML
footer_html = """
<div class='footer-info'>
    <p>âš ï¸ <strong>é‡è¦æé†’</strong>ï¼šæœ¬æœå‹™ç”± AI é©…å‹•ï¼Œå›ç­”åƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæ­£å¼æ³•å¾‹æ„è¦‹</p>
    <p class='footer-credits'>ğŸš€ Powered by API Â· ğŸ’š Built with Gradio</p>
</div>
"""

adaptive_css = """
/* ========== æ·±æ·ºè‰²æ¨¡å¼è®Šæ•¸ (â˜… å·²æ›´æ–°ç‚º EY è‰²ç³») ========== */
:root {
    /* â˜… ä¿®æ­£ï¼šèƒŒæ™¯æ”¹ç‚ºä¸­æ€§æ·ºç°ç™½ */
    --bg-gradient-start: #f5f5f5;
    --bg-gradient-end: #ffffff;
    --card-bg: #ffffff;
    --card-border: #e2e8f0;
    --text-primary: #2E2E38; /* â˜… ä¿®æ­£ï¼šä¸»è¦æ–‡å­—æ”¹ç‚ºæ·±ç° */
    --text-secondary: #64748b;
    --text-tertiary: #94a3b8;
    --input-bg: #f8fafc;
    --input-border: #e2e8f0;
    --input-focus-border: #FFEB00; /* â˜… ä¿®æ­£ï¼šç„¦é»æ”¹ç‚º EY é»ƒ */
    --chat-bg: #fafafa;
    --bot-bubble-bg: #ffffff;
    --bot-bubble-border: #e2e8f0;
    --example-bg: #ffffff;
    --example-hover-bg: #fffdeB; /* â˜… ä¿®æ­£ï¼šç¯„ä¾‹ hover æ”¹ç‚ºæ·ºé»ƒ */
    --divider: #f1f5f9;
    --shadow-sm: rgba(0, 0, 0, 0.05);
    --shadow-md: rgba(0, 0, 0, 0.08);
    --shadow-lg: rgba(255, 235, 0, 0.4); /* â˜… ä¿®æ­£ï¼šé™°å½±æ”¹ç‚º EY é»ƒ */
}

@media (prefers-color-scheme: dark) {
    :root {
        --bg-gradient-start: #0f172a;
        --bg-gradient-end: #1e293b;
        --card-bg: #1e293b;
        --card-border: #334155;
        --text-primary: #f1f5f9;
        --text-secondary: #cbd5e1;
        --text-tertiary: #94a3b8;
        --input-bg: #334155;
        --input-border: #475569;
        --input-focus-border: #FFEB00; /* â˜… ä¿®æ­£ï¼šç„¦é»æ”¹ç‚º EY é»ƒ */
        --chat-bg: #0f172a;
        --bot-bubble-bg: #334155;
        --bot-bubble-border: #475569;
        --example-bg: #334155;
        --example-hover-bg: #3a3800; /* â˜… ä¿®æ­£ï¼šç¯„ä¾‹ hover æ”¹ç‚ºæ·±é»ƒ */
        --divider: #334155;
        --shadow-sm: rgba(0, 0, 0, 0.3);
        --shadow-md: rgba(0, 0, 0, 0.4);
        --shadow-lg: rgba(255, 235, 0, 0.3); /* â˜… ä¿®æ­£ï¼šé™°å½±æ”¹ç‚º EY é»ƒ */
    }
}

/* ========== å…¨åŸŸæ¨£å¼ (â˜… å·²æ›´æ–°èƒŒæ™¯) ========== */
.gradio-container {
    max-width: 1200px !important;
    margin: 0 auto !important;
    background: linear-gradient(to bottom, var(--bg-gradient-start), var(--bg-gradient-end)) !important;
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Noto Sans TC', sans-serif !important;
    min-height: 100vh !important;
    padding: 2rem 1rem !important;
}

/* ========== ä¸»å®¹å™¨ ========== */
.gradio-container > .contain {
    background: var(--card-bg) !important;
    border-radius: 24px !important;
    padding: 3rem 2.5rem !important;
    box-shadow: 0 20px 60px var(--shadow-md) !important;
    border: 1px solid var(--card-border) !important;
}

/* ========== æ¨™é¡Œå€ (â˜… å·²æ›´æ–°æ¨™é¡Œé¡è‰²) ========== */
.title-section {
    text-align: center;
    margin-bottom: 2.5rem;
    padding-bottom: 2rem;
    border-bottom: 2px solid var(--divider);
}
.main-title {
    font-size: 2.5rem;
    font-weight: 800;
    /* â˜… ä¿®æ­£ï¼šç§»é™¤æ¼¸å±¤ï¼Œæ”¹ç‚º EY æ·±ç°è‰² */
    color: #2E2E38 !important;
    background: none !important;
    -webkit-background-clip: initial !important;
    -webkit-text-fill-color: initial !important;
    background-clip: initial !important;
    margin: 0 0 1rem 0;
    letter-spacing: -0.02em;
}
.subtitle {
    font-size: 1.1rem;
    color: var(--text-secondary);
    line-height: 1.6;
    margin: 0;
}

/* ========== èŠå¤©å€åŸŸ (â˜… å·²æ›´æ–°ä½¿ç”¨è€…å°è©±æ¡†) ========== */
.gradio-container .chatbot {
    border: 2px solid var(--card-border) !important;
    border-radius: 20px !important;
    background: var(--chat-bg) !important;
    padding: 0 !important;
    position: relative;
    z-index: 1;
}
.gradio-container .message-wrap { padding: 1rem !important; }
.gradio-container [data-testid="user"] { justify-content: flex-end !important; }
.gradio-container [data-testid="user"] .message {
    /* â˜… ä¿®æ­£ï¼šä½¿ç”¨è€…å°è©±æ¡†æ”¹ç‚º EY æ·±ç°è‰² */
    background: #2E2E38 !important;
    color: white !important;
    border: none !important;
    border-radius: 20px 20px 4px 20px !important;
    padding: 0.75rem 1.25rem !important;
    box-shadow: 0 4px 12px var(--shadow-sm) !important; /* é™°å½±æ”¹ç‚ºä¸­æ€§ */
    max-width: 80% !important;
}
.gradio-container [data-testid="bot"] .message {
    background: var(--bot-bubble-bg) !important;
    color: var(--text-primary) !important;
    border: 2px solid var(--bot-bubble-border) !important;
    border-radius: 20px 20px 20px 4px !important;
    padding: 0.75rem 1.25rem !important;
    box-shadow: 0 2px 8px var(--shadow-sm) !important;
    max-width: 80% !important;
}

/* ========== è¼¸å…¥å€åŸŸ (â˜… å·²æ›´æ–°ç„¦é»é¡è‰²) ========== */
.input-row {
    display: flex !important;
    gap: 12px !important;
    align-items: stretch !important;
    margin-top: 1.5rem !important;
    position: relative;
    z-index: 10;
}
.input-row .gradio-textbox {
    flex: 6 1 0% !important;
}
.gradio-container textarea {
    background: var(--input-bg) !important;
    border: 2px solid var(--input-border) !important;
    border-radius: 16px !important;
    padding: 1rem 1.5rem !important;
    font-size: 1rem !important;
    color: var(--text-primary) !important;
    transition: all 0.2s ease !important;
    line-height: 1.5 !important;
    min-height: 56px !important;
}
.gradio-container textarea:focus {
    background: var(--card-bg) !important;
    border-color: var(--input-focus-border) !important; /* EY é»ƒ */
    outline: none !important;
    box-shadow: 0 0 0 4px rgba(255, 235, 0, 0.25) !important; /* EY é»ƒé™°å½± */
}
.gradio-container textarea::placeholder {
    color: var(--text-tertiary) !important;
}

/* ========== æŒ‰éˆ• (â˜… å·²æ›´æ–°ç™¼é€éˆ•é¡è‰²) ========== */
.input-row .gradio-button {
    flex: 1 1 0% !important;
    max-width: fit-content !important;
}
.gradio-container button {
    border-radius: 16px !important;
    font-weight: 600 !important;
    font-size: 1rem !important;
    padding: 1rem 2rem !important;
    transition: all 0.2s ease !important;
    cursor: pointer !important;
    border: none !important;
    white-space: nowrap !important;
    height: 100% !important;
}
.send-btn {
    /* â˜… ä¿®æ­£ï¼šç™¼é€éˆ•æ”¹ç‚º EY é»ƒï¼Œæ–‡å­—æ”¹ç‚ºæ·±ç° */
    background: #FFEB00 !important;
    color: #2E2E38 !important;
    box-shadow: 0 4px 12px var(--shadow-lg) !important; /* EY é»ƒé™°å½± */
}
.send-btn:hover {
    transform: translateY(-2px) !important;
    box-shadow: 0 6px 20px var(--shadow-lg) !important; /* EY é»ƒé™°å½± */
}
.clear-btn {
    background: var(--card-bg) !important;
    color: var(--text-secondary) !important;
    border: 2px solid var(--card-border) !important;
}
.clear-btn:hover {
    background: var(--input-bg) !important;
    color: var(--text-primary) !important;
}

/* ========== ç¯„ä¾‹å•é¡Œ (â˜… å·²æ›´æ–° hover é¡è‰²) ========== */
.examples-section {
    margin-top: 2rem;
    position: relative;
    z-index: 5;
}
.examples-section .gradio-label-wrap {
    padding: 0 !important;
    margin: 0 !important;
}
.examples-section .gradio-label-wrap label {
    font-size: 0.9rem !important;
    font-weight: 600 !important;
    color: var(--text-secondary) !important;
    margin-bottom: 1rem !important;
    text-transform: uppercase !important;
    letter-spacing: 0.05em !important;
    display: block !important;
}
.gradio-container .examples button {
    background: var(--example-bg) !important;
    border: 2px solid var(--card-border) !important;
    border-radius: 12px !important;
    padding: 0.875rem 1.25rem !important;
    color: var(--text-secondary) !important;
    font-weight: 500 !important;
    font-size: 0.95rem !important;
    text-align: left !important;
    transition: all 0.2s ease !important;
}
.gradio-container .examples button:hover {
    /* â˜… ä¿®æ­£ï¼šHover æ”¹ç‚º EY æ·ºé»ƒ */
    background: var(--example-hover-bg) !important;
    border-color: #FFEB00 !important; /* EY é»ƒ */
    color: #2E2E38 !important; /* æ·±ç°æ–‡å­— */
    transform: translateX(4px) !important;
}

/* ========== åº•éƒ¨è³‡è¨Š ========== */
.footer-info {
    text-align: center;
    margin-top: 2rem;
    padding-top: 2rem;
    border-top: 2px solid var(--divider);
    color: var(--text-secondary);
    position: relative;
    z-index: 5;
}
.footer-info p { margin: 0.5rem 0; font-size: 0.95rem; }
.footer-credits { opacity: 0.7; margin-top: 1rem !important; }

/* ========== éŸ¿æ‡‰å¼è¨­è¨ˆ ========== */
@media (max-width: 768px) {
    .gradio-container > .contain {
        padding: 2rem 1.5rem !important;
    }
    .main-title { font-size: 2rem !important; }
    .subtitle { font-size: 1rem !important; }
    .input-row { flex-wrap: wrap !important; }
    .input-row .gradio-textbox {
        flex-basis: 100% !important;
    }
    .input-row .gradio-button {
        flex: 1 !important;
    }
    .gradio-container button {
        min-width: 120px !important;
    }
}
"""

# è³‡æ–™è®€å–èˆ‡è™•ç†
def read_docx(filepath):
    if not os.path.exists(filepath):
        return []
    doc = Document(filepath)
    return [p.text.strip() for p in doc.paragraphs if p.text.strip()]

def normalize_text(s):
    s = s.replace('\u3000', ' ')
    s = re.sub(r'\s+', ' ', s)
    return s.strip()

# è®€å–æ¢æ–‡
try:
    with open("labor_law_articles", "r", encoding="utf-8") as f:
        law_articles = [normalize_text(line) for line in f.readlines() if normalize_text(line)]
except FileNotFoundError:
    law_articles = []

# è®€å– QA
law_qas_raw = read_docx("labor_law_qa.docx")

# è§£æ Q&A pairs
qa_pairs = []
i = 0
while i < len(law_qas_raw):
    para = law_qas_raw[i]
    if para.upper().startswith('Q:') or para.upper().startswith('Qï¼š'):
        current_q = para
        current_a_parts = []
        i += 1
        while i < len(law_qas_raw) and not (law_qas_raw[i].upper().startswith('Q:') or law_qas_raw[i].upper().startswith('Qï¼š')):
            a_part = law_qas_raw[i]
            if a_part:
                current_a_parts.append(a_part)
            i += 1
        if current_a_parts:
            combined_a = " ".join(current_a_parts)
            qa_pairs.append({"q": current_q, "a": combined_a})
        else:
            i += 1
    else:
        i += 1

# æœ€çµ‚åˆä½µç´¢å¼•æ–‡ä»¶ (åƒ…æ³•æ¢)
docs = law_articles

# æ–‡ä»¶åˆ‡å‰² (Chunking)
chunk_size = 512
stride = 512

chunks = []
for doc in docs:
    doc = normalize_text(doc)
    if not doc:
        continue
    if len(doc) <= chunk_size:
        chunks.append(doc)
    else:
        for i in range(0, len(doc), stride):
            chunk = doc[i:i + chunk_size]
            if chunk:
                chunks.append(chunk)

# Embedding æ¨¡å‹èˆ‡ FAISS ç´¢å¼•
embedding_model_name = "intfloat/multilingual-e5-base"
embedding_model = SentenceTransformer(embedding_model_name, device=device)

if chunks:
    chunks_embeddings = embedding_model.encode(
        chunks,
        show_progress_bar=True,
        convert_to_numpy=True
    )
    d = chunks_embeddings.shape[1]
    global_index = faiss.IndexFlatL2(d)
    global_index.add(chunks_embeddings)
else:
    global_index = None

# BM25 ç´¢å¼•
if chunks:
    tokenized_global_chunks = [list(jieba.cut(chunk)) for chunk in tqdm(chunks, desc="Tokenizing")]
    global_bm25_model = BM25Okapi(tokenized_global_chunks)
else:
    global_bm25_model = None

# æª¢ç´¢å‡½å¼ (BM25 & RRF)
def bm25_retrieve(query: str, chunks: list, bm25_model: BM25Okapi, top_k: int = 20) -> list:
    tokenized_query = list(jieba.cut(query))
    scores = bm25_model.get_scores(tokenized_query)
    top_n_indices = np.argsort(scores)[::-1][:top_k]
    return [chunks[i] for i in top_n_indices]

def reciprocal_rank_fusion(*ranked_lists, k=60) -> list:
    scores = {}
    if not ranked_lists or all(not lst for lst in ranked_lists):
        return []

    for rl in ranked_lists:
        if not rl: continue
        for rank, doc_id in enumerate(rl, start=1):
            if not isinstance(doc_id, str): doc_id = str(doc_id)
            scores[doc_id] = scores.get(doc_id, 0.0) + 1.0 / (k + rank)

    if not scores: return []
    fused = sorted(scores.items(), key=lambda x: (-x[1], x[0]))
    return [d for d, _ in fused]

# è¼‰å…¥ Gemma-3-4B-it
generator_model_name = "unsloth/gemma-3-4b-it"
tokenizer = AutoTokenizer.from_pretrained(generator_model_name)
generator_pipeline = pipeline(
    "text-generation",
    model=generator_model_name,
    tokenizer=tokenizer,
    model_kwargs={
        "torch_dtype": torch.bfloat16,
    },
    device_map="auto",
)
terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<end_of_turn>")
]

RELEVANCE_THRESHOLD = 1.0

# RAG æ ¸å¿ƒå‡½å¼ (Hybrid v2)
def ask_laborlaw_gemma_conversational_v2(
    query: str,
    chat_history: list,
    faiss_index: faiss.Index,
    chunk_list: list,
    emb_model: SentenceTransformer,
    bm25_model: BM25Okapi,
    top_k: int = 20,
    chunks_to_feed: int = 5,
    debug: bool = False
):
    if faiss_index is None or faiss_index.ntotal == 0 or not chunk_list or bm25_model is None:
        return "æŠ±æ­‰ï¼ŒçŸ¥è­˜åº«å°šæœªæº–å‚™å°±ç·’ã€‚", chat_history

    q_emb = emb_model.encode([query], convert_to_numpy=True)
    distances, indices = faiss_index.search(q_emb, top_k)
    best_distance = distances[0][0]
    is_relevant = best_distance < RELEVANCE_THRESHOLD

    system_prompt = ""
    user_message_content = ""

    if is_relevant:
        system_prompt = "ä½ æ˜¯ä¸€ä½ç†Ÿæ‚‰å°ç£ã€Šå‹å‹•åŸºæº–æ³•ã€‹çš„å°ˆæ¥­æ³•å¾‹åŠ©ç†ã€‚è«‹æ ¹æ“šæä¾›çš„å…§å®¹å›ç­”å•é¡Œï¼Œä¸å¯æ†‘ç©ºæé€ ã€‚è«‹ä»¥æ­£å¼ã€æ¢ç†åˆ†æ˜çš„ä¸­æ–‡å›ç­”ï¼Œä¸¦ç›¡å¯èƒ½é™„ä¸Šç›¸é—œæ³•æ¢ä¾æ“šã€‚"

        valid_indices = [i for i in indices[0] if i < len(chunk_list)]
        relevant_chunks_embedding = [chunk_list[i] for i in valid_indices]

        relevant_chunks_bm25 = bm25_retrieve(
            query=query,
            chunks=chunk_list,
            bm25_model=bm25_model,
            top_k=top_k
        )

        fused_chunks_list = reciprocal_rank_fusion(
            relevant_chunks_bm25,
            relevant_chunks_embedding
        )

        final_fused_chunks = fused_chunks_list[:chunks_to_feed]

        if not final_fused_chunks:
            relevant_chunks = "ï¼ˆRRF èåˆå¾Œæœªæª¢ç´¢åˆ°ç›¸é—œè³‡æ–™ï¼‰"
        else:
            relevant_chunks = "\n\n".join(final_fused_chunks)

        user_message_content = f"""
ä»¥ä¸‹æ˜¯å‹å‹•åŸºæº–æ³•ç›¸é—œè³‡æ–™ï¼š
---
{relevant_chunks}
---
è«‹æ ¹æ“šä¸Šé¢è³‡æ–™å›ç­”ä¸‹åˆ—å•é¡Œï¼š
{query}
"""
    else:
        system_prompt = """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ AI åŠ©ç†ï¼Œä½ çš„ "å”¯ä¸€" è·è²¬æ˜¯å›ç­”å°ç£ã€Šå‹å‹•åŸºæº–æ³•ã€‹çš„ç›¸é—œå•é¡Œã€‚
ä½  "çµ•å°ä¸å¯ä»¥" å›ç­”ä»»ä½•èˆ‡å‹å‹•åŸºæº–æ³•ç„¡é—œçš„å•é¡Œã€‚
å¦‚æœä½¿ç”¨è€…è©¢å•ç„¡é—œå•é¡Œï¼ˆä¾‹å¦‚ï¼šå¤©æ°£ã€é£Ÿè­œã€é–’èŠã€è›‹ç³•ï¼‰ï¼Œè«‹ä½ ç¦®è²Œåœ°æ‹’çµ•ï¼Œä¸¦æ¸…æ¥šèªªæ˜ä½ çš„å°ˆé•·æ˜¯å‹å‹•æ³•è¦ã€‚
"""
        user_message_content = query

    messages_to_send = chat_history.copy()
    if not chat_history:
        user_message_content = f"{system_prompt}\n\n{user_message_content}"
    messages_to_send.append({"role": "user", "content": user_message_content})

    prompt = tokenizer.apply_chat_template(
        messages_to_send,
        tokenize=False,
        add_generation_prompt=True
    )

    try:
        out_list = generator_pipeline(
            prompt,
            max_new_tokens=1024,
            do_sample=False,
            eos_token_id=terminators,
        )

        full_text = out_list[0]["generated_text"]
        answer = full_text[len(prompt):].strip()
        answer = answer.replace("<end_of_turn>", "").strip()

        if not answer:
            answer = "ï¼ˆæ¨¡å‹æ²’æœ‰ç”Ÿæˆä»»ä½•å›æ‡‰ï¼‰"

        return answer, chat_history

    except Exception as e:
        return f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}", chat_history

# K-Fold è©•ä¼°
N_SPLITS = 5

if qa_pairs:
    qa_pairs_array = np.array(qa_pairs)
else:
    qa_pairs_array = np.array([])

if qa_pairs_array.size > 0 and global_index is not None and global_bm25_model is not None:
    if len(qa_pairs_array) < N_SPLITS:
        N_SPLITS = len(qa_pairs_array)

    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)
    all_fold_scores = []
    all_fold_results = []

    for fold_num, (train_index_ignored, test_index) in enumerate(kf.split(qa_pairs_array)):
        evaluation_set = qa_pairs_array[test_index]
        fold_results = []

        for item in tqdm(evaluation_set, desc=f"Fold {fold_num+1} Evaluating"):
            question = item['q']
            ground_truth_answer = item['a']

            generated_answer, _ = ask_laborlaw_gemma_conversational_v2(
                query=question,
                chat_history=[],
                faiss_index=global_index,
                chunk_list=chunks,
                emb_model=embedding_model,
                bm25_model=global_bm25_model,
                top_k=20,
                chunks_to_feed=5,
                debug=False
            )

            fold_results.append({
                "question": question,
                "ground_truth": ground_truth_answer,
                "generated": generated_answer
            })

        ground_truth_list = [res['ground_truth'] for res in fold_results]
        generated_list = [res['generated'] for res in fold_results]

        truth_embeddings = embedding_model.encode(ground_truth_list, show_progress_bar=False)
        gen_embeddings = embedding_model.encode(generated_list, show_progress_bar=False)

        similarities = np.diag(cosine_similarity(truth_embeddings, gen_embeddings))
        average_similarity = np.mean(similarities)

        all_fold_scores.append(average_similarity)
        all_fold_results.extend(fold_results)

    final_mean = np.mean(all_fold_scores)
    final_std = np.std(all_fold_scores)

# Gradio è¼”åŠ©å‡½å¼
def handle_submit(message, history_tuples):
    chat_history_for_gemma = []
    for user_msg, bot_msg in history_tuples:
        if user_msg:
            chat_history_for_gemma.append({"role": "user", "content": user_msg})
        if bot_msg:
            chat_history_for_gemma.append({"role": "assistant", "content": bot_msg})

    try:
        response_text, _ = ask_laborlaw_gemma_conversational_v2(
            query=message,
            chat_history=chat_history_for_gemma,
            faiss_index=global_index,
            chunk_list=chunks,
            emb_model=embedding_model,
            bm25_model=global_bm25_model,
            top_k=20,
            chunks_to_feed=5,
            debug=False
        )
    except Exception as e:
        response_text = f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

    history_tuples.append([message, response_text])
    return "", history_tuples

def clear_conversation():
    return "", []

# å»ºæ§‹ä¸¦å•Ÿå‹• Gradio ä»‹é¢
with gr.Blocks(css=adaptive_css, title="å®‰æ°¸éŠ€è¡Œå‹å‹•æ¬Šç›Šå°åŠ©æ‰‹", elem_classes="contain") as demo:
    gr.HTML("""
        <div class='title-section'>
            <h1 class='main-title'>ğŸ¢ å®‰æ°¸éŠ€è¡Œå‹å‹•æ¬Šç›Šå°åŠ©æ‰‹</h1>
            <p class='subtitle'>æ‚¨çš„å°ˆå±¬å‹å‹•æ³•å¾‹é¡§å• Â· å¿«é€Ÿã€æº–ç¢ºã€æ˜“æ‡‚çš„æ³•è¦è«®è©¢æœå‹™</p>
        </div>
    """)

    chatbot = gr.Chatbot(
        type="messages",
        height=800,
        show_copy_button=True,
        avatar_images=(None, None),
        elem_classes="chatbot"
    )

    with gr.Row(elem_classes="input-row"):
        message = gr.Textbox(
            placeholder="ğŸ’¬ è«‹è¼¸å…¥æ‚¨çš„å‹å‹•æ³•è¦å•é¡Œ...",
            show_label=False,
            scale=6,
            lines=2,
            autoscroll=True
        )
        send_btn = gr.Button("ç™¼é€", elem_classes="send-btn", scale=1)
        clear_btn = gr.Button("æ¸…é™¤", elem_classes="clear-btn", scale=1)

    with gr.Column(elem_classes="examples-section"):
        gr.Examples(
            label="ğŸ’¡ å¸¸è¦‹å•é¡Œ",
            examples=[
                ["å‹å·¥æ¯å¤©æœ€é•·å¯ä»¥å·¥ä½œå¹¾å°æ™‚ï¼Ÿ"],
                ["ä¸€å€‹æœˆæœ€å¤šå¯ä»¥åŠ ç­å¤šä¹…ï¼Ÿ"],
                ["åŠ ç­è²»æ‡‰è©²å¦‚ä½•è¨ˆç®—ï¼Ÿ"],
                ["ç‰¹ä¼‘å‡çš„è¦å®šæ˜¯ä»€éº¼ï¼Ÿ"]
            ],
            inputs=message
        )

    gr.HTML(footer_html)

    message.submit(handle_submit, [message, chatbot], [message, chatbot])
    send_btn.click(handle_submit, [message, chatbot], [message, chatbot])
    clear_btn.click(clear_conversation, None, [message, chatbot])

demo.launch(share=True, debug=True)